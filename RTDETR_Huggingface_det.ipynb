{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "%cd /mydrive/Hugging Face\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRZDeb8xeq6T",
        "outputId": "0a810ecf-57a6-44ab-bc01-17be0855410b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/.shortcut-targets-by-id/1OAwQPoxfnPPp90tlzRE1fTmfB377ksLA/Hugging Face\n",
            "best_vitpose_model\t\t   SegFormer_Huggingface_seg_train.ipynb\n",
            "datasets\t\t\t   segformer_trained_weights\n",
            "DETR_Huggingface_det.ipynb\t   segformer_trained_weights_47\n",
            "markerpensseg\t\t\t   ViT_Huggingface_classify.ipynb\n",
            "Markerpens_seg\t\t\t   vit-multilabel-best\n",
            "mask2former_Huggingface_seg.ipynb  ViTPose_Huggingface_keypoints.ipynb\n",
            "mask2formerresults\t\t   YOLOS_Huggingface_det.ipynb\n",
            "mydrive\t\t\t\t   YOLOS_Huggingface_det_wandb.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images(image_dir, exts={\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}):\n",
        "    image_count = 0\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if os.path.splitext(filename.lower())[1] in exts:\n",
        "            image_count += 1\n",
        "    print(f\"Total images: {image_count}\")\n",
        "\n",
        "count_images(\"datasets/surgical_instruments/images/train\")\n",
        "count_images(\"datasets/surgical_instruments/images/valid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euSiZjAmpI7e",
        "outputId": "60d946ad-ffff-4fc5-86e1-801e98e20b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 2855\n",
            "Total images: 841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def convert_to_detection_coco(input_path, output_path):\n",
        "    with open(input_path, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    detection_annotations = []\n",
        "    for ann in coco_data['annotations']:\n",
        "        # Remove segmentation-related fields\n",
        "        det_ann = {\n",
        "            'id': ann['id'],\n",
        "            'image_id': ann['image_id'],\n",
        "            'category_id': ann['category_id'],\n",
        "            'bbox': ann['bbox'],\n",
        "            'area': ann['area'],\n",
        "            'iscrowd': ann.get('iscrowd', 0)\n",
        "        }\n",
        "        detection_annotations.append(det_ann)\n",
        "\n",
        "    detection_coco = {\n",
        "        'images': coco_data['images'],\n",
        "        'annotations': detection_annotations,\n",
        "        'categories': coco_data['categories']\n",
        "    }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(detection_coco, f)\n",
        "\n",
        "    print(f\"Saved detection JSON to: {output_path}\")\n",
        "    print(f\"Images: {len(detection_coco['images'])}, Annotations: {len(detection_coco['annotations'])}\")\n",
        "\n",
        "# Paths\n",
        "train_input = \"datasets/surgical_instruments/annotations/instances_train.json\"\n",
        "valid_input = \"datasets/surgical_instruments/annotations/instances_valid.json\"\n",
        "train_output = \"datasets/surgical_instruments/annotations/instances_train_det.json\"\n",
        "valid_output = \"datasets/surgical_instruments/annotations/instances_valid_det.json\"\n",
        "\n",
        "# Run conversion\n",
        "convert_to_detection_coco(train_input, train_output)\n",
        "convert_to_detection_coco(valid_input, valid_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdEOJxketbGL",
        "outputId": "ad13a578-87dc-4be9-f9ba-01ae4f504563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved detection JSON to: datasets/surgical_instruments/annotations/instances_train_det.json\n",
            "Images: 2855, Annotations: 4895\n",
            "Saved detection JSON to: datasets/surgical_instruments/annotations/instances_valid_det.json\n",
            "Images: 841, Annotations: 1215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check train annotations\n",
        "with open(\"datasets/surgical_instruments/annotations/instances_train.json\") as f:\n",
        "    train_data = json.load(f)\n",
        "    print(f\"Train images: {len(train_data['images'])}\")\n",
        "    print(f\"Train annotations: {len(train_data['annotations'])}\")\n",
        "    print(f\"Categories: {train_data['categories']}\")\n",
        "\n",
        "# Check validation\n",
        "with open(\"datasets/surgical_instruments/annotations/instances_valid.json\") as f:\n",
        "    valid_data = json.load(f)\n",
        "    print(f\"\\nValid images: {len(valid_data['images'])}\")\n",
        "    print(f\"Valid annotations: {len(valid_data['annotations'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSYZrOocoieO",
        "outputId": "ba7a8c50-077e-4130-85d2-c3d5cbd5e145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 2855\n",
            "Train annotations: 4895\n",
            "Categories: [{'id': 1, 'name': 'Grasper'}, {'id': 2, 'name': 'Harmonic_Ace'}, {'id': 3, 'name': 'Myoma_Screw'}, {'id': 4, 'name': 'Needle_Holder'}, {'id': 5, 'name': 'Suction'}, {'id': 6, 'name': 'Trocar'}]\n",
            "\n",
            "Valid images: 841\n",
            "Valid annotations: 1215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "\n",
        "def check_coco_json(ann_file_path):\n",
        "    with open(ann_file_path, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    # Check top-level keys\n",
        "    assert 'images' in coco_data, \"'images' key not found in JSON\"\n",
        "    assert 'annotations' in coco_data, \"'annotations' key not found in JSON\"\n",
        "    assert 'categories' in coco_data, \"'categories' key not found in JSON\"\n",
        "\n",
        "    print(f\"Total images: {len(coco_data['images'])}\")\n",
        "    print(f\"Total annotations: {len(coco_data['annotations'])}\")\n",
        "    print(f\"Total categories: {len(coco_data['categories'])}\")\n",
        "\n",
        "    print(\"\\nSample category definitions:\")\n",
        "    pprint(coco_data['categories'])\n",
        "\n",
        "    # Check required annotation keys (excluding segmentation)\n",
        "    required_ann_keys = {\"id\", \"image_id\", \"bbox\", \"category_id\"}\n",
        "    missing_fields = []\n",
        "\n",
        "    for i, ann in enumerate(coco_data['annotations']):\n",
        "        if not required_ann_keys.issubset(ann):\n",
        "            missing_fields.append(i)\n",
        "\n",
        "    if missing_fields:\n",
        "        print(f\"‚ö†Ô∏è Missing required fields in {len(missing_fields)} annotations. Examples: {missing_fields[:5]}\")\n",
        "    else:\n",
        "        print(\"‚úÖ All annotations have required fields: 'id', 'image_id', 'bbox', 'category_id'\")\n",
        "\n",
        "    # Optional: Check iscrowd and area fields\n",
        "    optional_fields = [\"iscrowd\", \"area\"]\n",
        "    count_missing = defaultdict(int)\n",
        "\n",
        "    for ann in coco_data['annotations']:\n",
        "        for field in optional_fields:\n",
        "            if field not in ann:\n",
        "                count_missing[field] += 1\n",
        "\n",
        "    for field in optional_fields:\n",
        "        if count_missing[field]:\n",
        "            print(f\"‚ö†Ô∏è Missing '{field}' in {count_missing[field]} annotations\")\n",
        "        else:\n",
        "            print(f\"‚úÖ All annotations have '{field}'\")\n",
        "\n",
        "# Example usage:\n",
        "check_coco_json(\"datasets/surgical_instruments/annotations/instances_train_det.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7dZJPQAuI-L",
        "outputId": "80527c7b-7843-4303-811a-da7e03233380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 2855\n",
            "Total annotations: 4895\n",
            "Total categories: 6\n",
            "\n",
            "Sample category definitions:\n",
            "[{'id': 1, 'name': 'Grasper'},\n",
            " {'id': 2, 'name': 'Harmonic_Ace'},\n",
            " {'id': 3, 'name': 'Myoma_Screw'},\n",
            " {'id': 4, 'name': 'Needle_Holder'},\n",
            " {'id': 5, 'name': 'Suction'},\n",
            " {'id': 6, 'name': 'Trocar'}]\n",
            "‚úÖ All annotations have required fields: 'id', 'image_id', 'bbox', 'category_id'\n",
            "‚úÖ All annotations have 'iscrowd'\n",
            "‚úÖ All annotations have 'area'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def check_and_clean_coco_annotations(annotation_file, remove_invalid=False):\n",
        "    \"\"\"\n",
        "    Checks bounding boxes in COCO annotation JSON for:\n",
        "      - NaN values\n",
        "      - zero or negative width/height\n",
        "\n",
        "    Args:\n",
        "      annotation_file (str): Path to COCO-style JSON annotation file\n",
        "      remove_invalid (bool): If True, remove invalid annotations from JSON in memory\n",
        "\n",
        "    Returns:\n",
        "      invalid_ann_count (int): Number of invalid annotations found\n",
        "      cleaned_annotations (dict): Cleaned COCO dict (if remove_invalid=True)\n",
        "    \"\"\"\n",
        "    with open(annotation_file, 'r') as f:\n",
        "        coco = json.load(f)\n",
        "\n",
        "    invalid_ann_count = 0\n",
        "    valid_annotations = []\n",
        "\n",
        "    for ann in coco['annotations']:\n",
        "        bbox = ann.get('bbox', None)\n",
        "        if bbox is None:\n",
        "            print(f\"Annotation {ann.get('id', 'unknown')} missing bbox\")\n",
        "            invalid_ann_count += 1\n",
        "            continue\n",
        "\n",
        "        # Check for NaN in bbox\n",
        "        if any([v != v for v in bbox]):  # NaN check: v != v is True only if v is NaN\n",
        "            print(f\"Invalid bbox (NaN) in annotation {ann.get('id', 'unknown')}: {bbox}\")\n",
        "            invalid_ann_count += 1\n",
        "            continue\n",
        "\n",
        "        x, y, w, h = bbox\n",
        "        if w <= 0 or h <= 0:\n",
        "            print(f\"Invalid bbox (zero/negative size) in annotation {ann.get('id', 'unknown')}: {bbox}\")\n",
        "            invalid_ann_count += 1\n",
        "            continue\n",
        "\n",
        "        # Passed all checks, keep annotation\n",
        "        valid_annotations.append(ann)\n",
        "\n",
        "    if remove_invalid:\n",
        "        print(f\"Removed {invalid_ann_count} invalid annotations.\")\n",
        "        coco['annotations'] = valid_annotations\n",
        "        return invalid_ann_count, coco\n",
        "    else:\n",
        "        print(f\"Found {invalid_ann_count} invalid annotations.\")\n",
        "        return invalid_ann_count, None\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "annotation_path = \"datasets/surgical_instruments/annotations/instances_train_det.json\"\n",
        "\n",
        "invalid_count, cleaned_coco = check_and_clean_coco_annotations(annotation_path, remove_invalid=False)\n",
        "\n",
        "print(f\"Total invalid annotations: {invalid_count}\")\n",
        "\n",
        "# If you want to remove and save cleaned JSON:\n",
        "if invalid_count > 0:\n",
        "    _, cleaned = check_and_clean_coco_annotations(annotation_path, remove_invalid=True)\n",
        "    cleaned_path = annotation_path.replace(\".json\", \"_cleaned.json\")\n",
        "    with open(cleaned_path, 'w') as f_out:\n",
        "        json.dump(cleaned, f_out)\n",
        "    print(f\"Saved cleaned annotations to {cleaned_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE_5Yh5_S5Dk",
        "outputId": "ca6af7f2-67a1-4c84-b1a6-5f687a7a7a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 invalid annotations.\n",
            "Total invalid annotations: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L25ieLxa3gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35283aaa-cc78-427b-dfe1-4d2c142d63a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets torchvision pycocotools\n",
        "!pip install -q accelerate evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "token = userdata.get('Nyi_token')\n",
        "login(token=token)"
      ],
      "metadata": {
        "id": "UpGxOdRbi3F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import json\n",
        "from transformers import AutoProcessor, AutoModelForObjectDetection, TrainingArguments, Trainer\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "VyvdYTg8_k-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SurgicalInstrumentDetDataset(Dataset):\n",
        "    def __init__(self, images_dir, annotation_file, transforms=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        with open(annotation_file) as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        self.image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in coco[\"images\"]}\n",
        "        self.id_to_annotations = {}\n",
        "        for ann in coco[\"annotations\"]:\n",
        "            img_id = ann[\"image_id\"]\n",
        "            self.id_to_annotations.setdefault(img_id, []).append(ann)\n",
        "        self.img_ids = list(self.image_id_to_filename.keys())\n",
        "        self.categories = {cat[\"id\"]: cat[\"name\"] for cat in coco[\"categories\"]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        img_path = os.path.join(self.images_dir, self.image_id_to_filename[img_id])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        annotations = self.id_to_annotations.get(img_id, [])\n",
        "        target = {\n",
        "            \"image_id\": torch.tensor([img_id]),\n",
        "            \"annotations\": annotations\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "ZvTS3khQ_rsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"PekingU/rtdetr_r101vd_coco_o365\"\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForObjectDetection.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "lJIwwcDMo6qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RTDETRProcessedDataset(Dataset):\n",
        "    def __init__(self, base_dataset, processor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, target = self.base_dataset[idx]\n",
        "\n",
        "        # Wrap annotations dict for a single image inside a list\n",
        "        encoding = self.processor(\n",
        "            images=image,\n",
        "            annotations=[target],  # üëà fix: wrap in list\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze(0)\n",
        "        encoding[\"labels\"] = encoding[\"labels\"][0]\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "SwbGQIvrXcFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SurgicalInstrumentDetDataset(\n",
        "    images_dir=\"datasets/surgical_instruments/images/train\",\n",
        "    annotation_file=\"datasets/surgical_instruments/annotations/instances_train_det.json\"\n",
        ")\n",
        "\n",
        "valid_dataset = SurgicalInstrumentDetDataset(\n",
        "    images_dir=\"datasets/surgical_instruments/images/valid\",\n",
        "    annotation_file=\"datasets/surgical_instruments/annotations/instances_valid_det.json\"\n",
        ")\n",
        "\n",
        "train_processed = RTDETRProcessedDataset(train_dataset, processor)\n",
        "valid_processed = RTDETRProcessedDataset(valid_dataset, processor)"
      ],
      "metadata": {
        "id": "OcI1wgy12ItO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RTDETRDataCollator:\n",
        "    def __call__(self, features):\n",
        "        pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ],
      "metadata": {
        "id": "ShznjXQrCXsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/tmp/detr-surgical\",     # Temporary checkpoint dir\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    per_device_train_batch_size=16,       # Adjusted to fit typical Colab GPU memory\n",
        "    per_device_eval_batch_size=32,\n",
        "    fp16=True,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,                   # Keep only 1 checkpoint\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",   # DETR default metric (loss)\n",
        "    greater_is_better=False,              # Lower loss is better\n",
        "\n",
        "    logging_strategy=\"no\",                # Disable logging\n",
        "    report_to=\"none\",                     # No wandb/tensorboard\n",
        "\n",
        "    num_train_epochs=50,\n",
        "    dataloader_num_workers=4\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_processed,\n",
        "    eval_dataset=valid_processed,\n",
        "    tokenizer=processor,\n",
        "    data_collator=RTDETRDataCollator(),\n",
        ")\n"
      ],
      "metadata": {
        "id": "m6HQ4VCHYsVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58232ce3-bbf6-4a6e-ea67-173a50af71d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-29-3115622598.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "67gwfva52jNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"datasets/detr_surgical_best_model50\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IoAc3mUwy1WC",
        "outputId": "28775a9f-ec2f-4c3b-9282-682fed2528e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8950' max='8950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8950/8950 1:29:05, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.707376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.654958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.200691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.618654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.076127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.036953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.149938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.440506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.385855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.598637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.161190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.756713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.818869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.057231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.428293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.267798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.897130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.414618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.983212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.101680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.925524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.507948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.583906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.647572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.137710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.099021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.873340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.783762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.764329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.503049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.986536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.780570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.715521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.427386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.025948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.869569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.355244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.606627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.601164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.248521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.309415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.248299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.094335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.998425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.114490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.038063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.997068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.006861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.944170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.980920</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias', 'bbox_embed.0.layers.0.weight', 'bbox_embed.0.layers.0.bias', 'bbox_embed.0.layers.1.weight', 'bbox_embed.0.layers.1.bias', 'bbox_embed.0.layers.2.weight', 'bbox_embed.0.layers.2.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.2.weight', 'bbox_embed.5.layers.2.bias'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"datasets/detr_surgical_best_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1q7toQF6a3Fd",
        "outputId": "885101d7-58a1-4d0a-e3fc-d9f9a44c74ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5370' max='5370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5370/5370 56:45, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>11.647306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>12.251397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>11.116899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>11.804318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.595315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>11.831823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.363395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.424090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.370606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>11.301422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.699321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.006581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.497424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.670039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.069342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.105705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.910563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>10.219392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.765226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.275618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.936250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.842818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.219167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.042583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.713632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.881037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.824829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.762216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.716367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.572086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias', 'bbox_embed.0.layers.0.weight', 'bbox_embed.0.layers.0.bias', 'bbox_embed.0.layers.1.weight', 'bbox_embed.0.layers.1.bias', 'bbox_embed.0.layers.2.weight', 'bbox_embed.0.layers.2.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.2.weight', 'bbox_embed.5.layers.2.bias'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def run_per_class_eval(model, val_dataset, processor, annotation_file, class_names, device=\"cuda\", threshold=0.1):\n",
        "    \"\"\"\n",
        "    Runs COCO bbox evaluation per class for RTDETR model.\n",
        "\n",
        "    Args:\n",
        "        model: RTDETR model.\n",
        "        val_dataset: Validation dataset; must either have 'base_dataset' attribute returning (PIL image, target)\n",
        "                     or provide preprocessed tensors under 'pixel_values'.\n",
        "        processor: RTDETR processor to preprocess images and postprocess outputs.\n",
        "        annotation_file: Path to COCO-format ground truth annotation JSON.\n",
        "        class_names: List of class names matching COCO category names in order of category IDs.\n",
        "                     For example: ['no', 'Grasper', 'Harmonic_Ace', ...]\n",
        "        device: Device string, e.g., 'cuda' or 'cpu'.\n",
        "        threshold: Detection score threshold.\n",
        "\n",
        "    Returns:\n",
        "        None, prints per-class and overall COCO eval metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load COCO ground truth\n",
        "    coco_gt = COCO(annotation_file)\n",
        "\n",
        "    # Build a mapping from category name to category ID (COCO IDs)\n",
        "    cats = coco_gt.loadCats(coco_gt.getCatIds())\n",
        "    name_to_catid = {cat[\"name\"]: cat[\"id\"] for cat in cats}\n",
        "\n",
        "    # Map model class index (based on class_names list) to COCO category ID\n",
        "    class_idx_to_catid = {}\n",
        "    for idx, cname in enumerate(class_names):\n",
        "        if cname in name_to_catid:\n",
        "            class_idx_to_catid[idx] = name_to_catid[cname]\n",
        "        else:\n",
        "            class_idx_to_catid[idx] = None\n",
        "\n",
        "    # Inverse map: category ID -> class index for evaluation printing\n",
        "    catid_to_class_idx = {v: k for k, v in class_idx_to_catid.items() if v is not None}\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    results = []\n",
        "    for idx, item in enumerate(tqdm(val_dataset, desc=\"Evaluating\")):\n",
        "        if hasattr(val_dataset, \"base_dataset\"):\n",
        "            pil_img, labels = val_dataset.base_dataset[idx]\n",
        "            pixel_values = processor(images=pil_img, return_tensors=\"pt\").pixel_values.to(device)\n",
        "            height, width = pil_img.height, pil_img.width\n",
        "            image_id = labels.get(\"image_id\", idx) if isinstance(labels, dict) else idx\n",
        "        else:\n",
        "            pixel_values = item[\"pixel_values\"].unsqueeze(0).to(device)\n",
        "            labels = item[\"labels\"]\n",
        "            height, width = labels[\"size\"].tolist()\n",
        "            image_id = labels[\"image_id\"].item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(pixel_values)\n",
        "\n",
        "        processed = processor.post_process_object_detection(\n",
        "            outputs=outputs,\n",
        "            target_sizes=torch.tensor([[height, width]], device=device),\n",
        "            threshold=threshold\n",
        "        )[0]\n",
        "\n",
        "        for score, label, box in zip(processed[\"scores\"], processed[\"labels\"], processed[\"boxes\"]):\n",
        "            class_idx = label.item()\n",
        "            category_id = class_idx_to_catid.get(class_idx, None)\n",
        "            if category_id is None:\n",
        "                continue\n",
        "\n",
        "            x_min, y_min, x_max, y_max = box.tolist()\n",
        "            bbox = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]\n",
        "\n",
        "            results.append({\n",
        "                \"image_id\": int(image_id),\n",
        "                \"category_id\": int(category_id),\n",
        "                \"bbox\": bbox,\n",
        "                \"score\": float(score.item())\n",
        "            })\n",
        "\n",
        "    if len(results) == 0:\n",
        "        print(\"‚ö†Ô∏è No predictions found above threshold. Try lowering threshold or check preprocessing.\")\n",
        "        return\n",
        "\n",
        "    pred_file = \"datasets/rtdetr_predictions.json\"\n",
        "    with open(pred_file, \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    with open(annotation_file, \"r\") as f:\n",
        "        coco_data = json.load(f)\n",
        "    coco_data.setdefault(\"info\", {\"description\": \"patched\"})\n",
        "    coco_data.setdefault(\"licenses\", [{\"id\": 0, \"name\": \"none\", \"url\": \"\"}])\n",
        "    patched_gt_file = \"datasets/rtdetr_patched_gt.json\"\n",
        "    with open(patched_gt_file, \"w\") as f:\n",
        "        json.dump(coco_data, f)\n",
        "\n",
        "    coco_gt_eval = COCO(patched_gt_file)\n",
        "    coco_dt_eval = coco_gt_eval.loadRes(pred_file)\n",
        "\n",
        "    coco_eval = COCOeval(coco_gt_eval, coco_dt_eval, iouType='bbox')\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    precisions = coco_eval.eval['precision']\n",
        "    recalls = coco_eval.eval['recall']\n",
        "\n",
        "    print(\"\\nüìä Per-class Evaluation (IoU=0.50:0.95):\")\n",
        "    print(f\"{'Class':<5} {'Name':<20} {'Precision':>9} {'Recall':>9} {'F1 Score':>9}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    per_class_p, per_class_r, per_class_f1 = [], [], []\n",
        "\n",
        "    for cat_id in coco_eval.params.catIds:\n",
        "        if cat_id not in catid_to_class_idx:\n",
        "            continue\n",
        "\n",
        "        i = catid_to_class_idx[cat_id]\n",
        "        cat_idx = coco_eval.params.catIds.index(cat_id)\n",
        "\n",
        "        precision = precisions[:, :, cat_idx, 0, 0]\n",
        "        recall = recalls[:, cat_idx, 0, 0]\n",
        "\n",
        "        valid_p = precision[precision > -1]\n",
        "        valid_r = recall[recall > -1]\n",
        "\n",
        "        p = np.mean(valid_p) if valid_p.size > 0 else 0.0\n",
        "        r = np.mean(valid_r) if valid_r.size > 0 else 0.0\n",
        "        f1 = 2 * p * r / (p + r + 1e-8) if (p + r) > 0 else 0.0\n",
        "\n",
        "        per_class_p.append(p)\n",
        "        per_class_r.append(r)\n",
        "        per_class_f1.append(f1)\n",
        "\n",
        "        print(f\"{i:<5} {class_names[i]:<20} {p:9.4f} {r:9.4f} {f1:9.4f}\")\n",
        "\n",
        "    print(\"\\nüîç Overall Average:\")\n",
        "    print(f\"Precision: {np.mean(per_class_p):.4f}\")\n",
        "    print(f\"Recall:    {np.mean(per_class_r):.4f}\")\n",
        "    print(f\"F1 Score:  {np.mean(per_class_f1):.4f}\")"
      ],
      "metadata": {
        "id": "uZJ4ZDr0FYr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 50 epochs\n",
        "model_name = \"PekingU/rtdetr_r101vd_coco_o365\"\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForObjectDetection.from_pretrained(\"datasets/detr_surgical_best_model50\")\n",
        "\n",
        "run_per_class_eval(\n",
        "    model=model,\n",
        "    val_dataset=valid_processed,  # RTDETRProcessedDataset\n",
        "    processor=processor,\n",
        "    annotation_file=\"datasets/surgical_instruments/annotations/instances_valid_det.json\",\n",
        "    device=\"cuda\",\n",
        "    class_names = ['no', 'Grasper', 'Harmonic_Ace', 'Myoma_Screw',\n",
        "               'Needle_Holder', 'Suction', 'Trocar']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMoVVTUdhlaa",
        "outputId": "c9357ac5-0c58-453e-b185-130556d77088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 841/841 [01:56<00:00,  7.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.87s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.780\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.864\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.838\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.578\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.783\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.740\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.828\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.828\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.588\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.834\n",
            "\n",
            "üìä Per-class Evaluation (IoU=0.50:0.95):\n",
            "Class Name                 Precision    Recall  F1 Score\n",
            "-------------------------------------------------------\n",
            "1     Grasper                 0.6845    0.7235    0.7035\n",
            "2     Harmonic_Ace            0.8759    0.9050    0.8902\n",
            "3     Myoma_Screw             0.7757    0.8275    0.8008\n",
            "4     Needle_Holder           0.5642    0.5947    0.5790\n",
            "5     Suction                 0.5811    0.6391    0.6087\n",
            "6     Trocar                  0.7259    0.7525    0.7390\n",
            "\n",
            "üîç Overall Average:\n",
            "Precision: 0.7012\n",
            "Recall:    0.7404\n",
            "F1 Score:  0.7202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"PekingU/rtdetr_r101vd_coco_o365\"\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForObjectDetection.from_pretrained(\"datasets/detr_surgical_best_model\")\n",
        "\n",
        "run_per_class_eval(\n",
        "    model=model,\n",
        "    val_dataset=valid_processed,  # RTDETRProcessedDataset\n",
        "    processor=processor,\n",
        "    annotation_file=\"datasets/surgical_instruments/annotations/instances_valid_det.json\",\n",
        "    device=\"cuda\",\n",
        "    class_names = ['no', 'Grasper', 'Harmonic_Ace', 'Myoma_Screw',\n",
        "               'Needle_Holder', 'Suction', 'Trocar']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMlsFxYjFoaB",
        "outputId": "c9303c01-efba-4db5-8826-bb961a02169f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 841/841 [01:44<00:00,  8.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.76s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.718\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.792\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.764\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.428\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.723\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.709\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.778\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.778\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.512\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.784\n",
            "\n",
            "üìä Per-class Evaluation (IoU=0.50:0.95):\n",
            "Class Name                 Precision    Recall  F1 Score\n",
            "-------------------------------------------------------\n",
            "1     Grasper                 0.6633    0.6965    0.6795\n",
            "2     Harmonic_Ace            0.8137    0.8720    0.8418\n",
            "3     Myoma_Screw             0.7778    0.8520    0.8132\n",
            "4     Needle_Holder           0.4880    0.5457    0.5152\n",
            "5     Suction                 0.4963    0.5478    0.5208\n",
            "6     Trocar                  0.7157    0.7400    0.7276\n",
            "\n",
            "üîç Overall Average:\n",
            "Precision: 0.6591\n",
            "Recall:    0.7090\n",
            "F1 Score:  0.6830\n"
          ]
        }
      ]
    }
  ]
}